import XCTest
@testable import MLXEngine

#if canImport(MLX) && canImport(MLXLLM) && canImport(MLXLMCommon)
import MLX
import MLXLLM
import MLXLMCommon

final class MLXIntegrationTests: XCTestCase {
    
    // MARK: - Test Configuration
    
    override func setUp() async throws {
        // Set up MLX GPU cache limit for testing
        #if canImport(MLX)
        MLX.GPU.set(cacheLimit: 512 * 1024 * 1024) // 512MB
        #endif
    }
    
    override func tearDown() async throws {
        // Clean up any loaded models
        // This will be handled by the engine's unload method
    }
    
    // MARK: - Feature Detection Tests
    
    func testFeatureDetection() throws {
        guard ProcessInfo.processInfo.environment["RUN_REAL_MODEL_TESTS"] != nil else {
            throw XCTSkip("Skipping real model test: RUN_REAL_MODEL_TESTS not set")
        }
        print("\nüîç [FEATURE TEST] Testing feature detection...")
        
        let features = InferenceEngine.supportedFeatures
        print("‚úÖ [FEATURE TEST] Available features: \(features)")
        
        // Test core features
        XCTAssertTrue(features.contains(.streamingGeneration), "Streaming generation should be available")
        XCTAssertTrue(features.contains(.conversationMemory), "Conversation memory should be available")
        XCTAssertTrue(features.contains(.performanceMonitoring), "Performance monitoring should be available")
        
        // Test MLX-specific features
        #if canImport(MLX) && canImport(MLXLLM) && canImport(MLXLMCommon)
        XCTAssertTrue(features.contains(.quantizationSupport), "Quantization support should be available with MLX")
        XCTAssertTrue(features.contains(.modelCaching), "Model caching should be available with MLX")
        XCTAssertTrue(features.contains(.customTokenizers), "Custom tokenizers should be available with MLX")
        #endif
        
        print("‚úÖ [FEATURE TEST] Feature detection completed successfully")
    }
    
    // MARK: - LLM Model Tests
    
    func testLLMModelDownloadAndInference() async throws {
        guard ProcessInfo.processInfo.environment["RUN_REAL_MODEL_TESTS"] != nil else {
            throw XCTSkip("Skipping real model test: RUN_REAL_MODEL_TESTS not set")
        }
        print("\nüöÄ [LLM TEST] Testing LLM model download and inference...")
        
        // Test with a small, fast model
        let config = ModelRegistry.qwen05B
        print("üìã [LLM TEST] Using model: \(config.name) (\(config.hubId))")
        
        let loadProgressCollector = ProgressCollector()
        let startTime = Date()
        
        // Load the model
        let engine = try await InferenceEngine.loadModel(config) { progress in
            Task {
                await loadProgressCollector.addProgress(progress)
            }
        }
        
        let loadTime = Date().timeIntervalSince(startTime)
        let loadProgress = await loadProgressCollector.getProgressValues()
        
        print("‚úÖ [LLM TEST] Model loaded successfully!")
        print("   - Load time: \(String(format: "%.2f", loadTime)) seconds")
        print("   - Progress points: \(loadProgress.count)")
        print("   - Final progress: \(loadProgress.last ?? 0.0)")
        
        // Test text generation
        let testPrompt = "Hello! Please respond with a short, friendly greeting."
        print("üìù [LLM TEST] Test prompt: \"\(testPrompt)\"")
        
        let generateStartTime = Date()
        let response = try await engine.generate(testPrompt, params: GenerateParams(maxTokens: 50, temperature: 0.7))
        let generateTime = Date().timeIntervalSince(generateStartTime)
        
        print("‚úÖ [LLM TEST] Text generation successful!")
        print("   - Response: \"\(response)\"")
        print("   - Generation time: \(String(format: "%.2f", generateTime)) seconds")
        
        XCTAssertFalse(response.isEmpty, "Response should not be empty")
        XCTAssertTrue(response.count > 10, "Response should be substantial")
        
        // Test streaming generation
        print("\nüîÑ [LLM TEST] Testing streaming generation...")
        let streamPrompt = "Tell me a very short story about a cat."
        print("üìù [LLM TEST] Stream prompt: \"\(streamPrompt)\"")
        
        let streamStartTime = Date()
        var streamedText = ""
        var chunkCount = 0
        
        for try await chunk in engine.stream(streamPrompt, params: GenerateParams(maxTokens: 100, temperature: 0.8)) {
            streamedText += chunk
            chunkCount += 1
            print("   Chunk \(chunkCount): \"\(chunk)\"")
        }
        
        let streamTime = Date().timeIntervalSince(streamStartTime)
        print("‚úÖ [LLM TEST] Streaming generation successful!")
        print("   - Total chunks: \(chunkCount)")
        print("   - Streamed text: \"\(streamedText)\"")
        print("   - Stream time: \(String(format: "%.2f", streamTime)) seconds")
        
        XCTAssertFalse(streamedText.isEmpty, "Streamed text should not be empty")
        XCTAssertTrue(chunkCount > 1, "Should have multiple chunks")
        
        // Test model unloading
        print("\nüßπ [LLM TEST] Testing model unloading...")
        engine.unload()
        print("‚úÖ [LLM TEST] Model unloaded successfully")
        
        print("‚úÖ [LLM TEST] LLM model test completed successfully!")
    }
    
    // MARK: - VLM Model Tests
    
    func testVLMModelDownloadAndInference() async throws {
        guard ProcessInfo.processInfo.environment["RUN_REAL_MODEL_TESTS"] != nil else {
            throw XCTSkip("Skipping real model test: RUN_REAL_MODEL_TESTS not set")
        }
        print("\nüñºÔ∏è [VLM TEST] Testing VLM model download and inference...")
        
        // Check if VLM features are available
        guard InferenceEngine.supportedFeatures.contains(.visionLanguageModels) else {
            print("‚ö†Ô∏è [VLM TEST] VLM features not available, skipping test")
            return
        }
        
        // Test with LLaVA model
        let config = ModelRegistry.llava16_3B
        print("üìã [VLM TEST] Using model: \(config.name) (\(config.hubId))")
        
        let loadProgressCollector = ProgressCollector()
        let startTime = Date()
        
        // Load the VLM model
        let engine = try await InferenceEngine.loadModel(config) { progress in
            Task {
                await loadProgressCollector.addProgress(progress)
            }
        }
        
        let loadTime = Date().timeIntervalSince(startTime)
        let loadProgress = await loadProgressCollector.getProgressValues()
        
        print("‚úÖ [VLM TEST] VLM model loaded successfully!")
        print("   - Load time: \(String(format: "%.2f", loadTime)) seconds")
        print("   - Progress points: \(loadProgress.count)")
        
        // Test VLM-specific features
        XCTAssertTrue(InferenceEngine.supportedFeatures.contains(.multiModalInput), "Multi-modal input should be available for VLM")
        
        // Test text-only generation (VLM models can also do text generation)
        let testPrompt = "Describe what you see in this image: [No image provided, please respond with a general description of your capabilities]"
        print("üìù [VLM TEST] Test prompt: \"\(testPrompt)\"")
        
        let response = try await engine.generate(testPrompt, params: .init(maxTokens: 100, temperature: 0.7))
        
        print("‚úÖ [VLM TEST] VLM text generation successful!")
        print("   - Response: \"\(response)\"")
        
        XCTAssertFalse(response.isEmpty, "VLM response should not be empty")
        
        // Test model unloading
        print("\nüßπ [VLM TEST] Testing VLM model unloading...")
        engine.unload()
        print("‚úÖ [VLM TEST] VLM model unloaded successfully")
        
        print("‚úÖ [VLM TEST] VLM model test completed successfully!")
    }
    
    // MARK: - Embedding Model Tests
    
    func testEmbeddingModelDownloadAndInference() async throws {
        guard ProcessInfo.processInfo.environment["RUN_REAL_MODEL_TESTS"] != nil else {
            throw XCTSkip("Skipping real model test: RUN_REAL_MODEL_TESTS not set")
        }
        print("\nüîó [EMBEDDING TEST] Testing embedding model download and inference...")
        
        // Check if embedding features are available
        guard InferenceEngine.supportedFeatures.contains(.embeddingModels) else {
            print("‚ö†Ô∏è [EMBEDDING TEST] Embedding features not available, skipping test")
            return
        }
        
        // Test with BGE model
        let config = ModelRegistry.bgeSmallEn
        print("üìã [EMBEDDING TEST] Using model: \(config.name) (\(config.hubId))")
        
        let loadProgressCollector = ProgressCollector()
        let startTime = Date()
        
        // Load the embedding model
        let engine = try await InferenceEngine.loadModel(config) { progress in
            Task { await loadProgressCollector.addProgress(progress) }
        }
        
        let loadTime = Date().timeIntervalSince(startTime)
        let loadProgress = await loadProgressCollector.getProgressValues()
        
        print("‚úÖ [EMBEDDING TEST] Embedding model loaded successfully!")
        print("   - Load time: \(String(format: "%.2f", loadTime)) seconds")
        print("   - Progress points: \(loadProgress.count)")
        
        // Test embedding-specific features
        XCTAssertTrue(InferenceEngine.supportedFeatures.contains(.batchProcessing), "Batch processing should be available for embeddings")
        
        // Test text embedding generation
        let testText = "This is a test sentence for embedding generation."
        print("üìù [EMBEDDING TEST] Test text: \"\(testText)\"")
        
        let response = try await engine.generate(testText, params: GenerateParams(maxTokens: 512, temperature: 0.0))
        
        print("‚úÖ [EMBEDDING TEST] Embedding generation successful!")
        print("   - Response length: \(response.count)")
        
        XCTAssertFalse(response.isEmpty, "Embedding response should not be empty")
        
        // Test model unloading
        print("\nüßπ [EMBEDDING TEST] Testing embedding model unloading...")
        engine.unload()
        print("‚úÖ [EMBEDDING TEST] Embedding model unloaded successfully")
        
        print("‚úÖ [EMBEDDING TEST] Embedding model test completed successfully!")
    }
    
    // MARK: - Diffusion Model Tests
    
    func testDiffusionModelDownloadAndInference() async throws {
        guard ProcessInfo.processInfo.environment["RUN_REAL_MODEL_TESTS"] != nil else {
            throw XCTSkip("Skipping real model test: RUN_REAL_MODEL_TESTS not set")
        }
        print("\nüé® [DIFFUSION TEST] Testing diffusion model download and inference...")
        
        // Check if diffusion features are available
        guard InferenceEngine.supportedFeatures.contains(.diffusionModels) else {
            print("‚ö†Ô∏è [DIFFUSION TEST] Diffusion features not available, skipping test")
            return
        }
        
        // Test with Stable Diffusion model
        let config = ModelRegistry.stableDiffusionXL
        print("üìã [DIFFUSION TEST] Using model: \(config.name) (\(config.hubId))")
        
        let loadProgressCollector = ProgressCollector()
        let startTime = Date()
        
        // Load the diffusion model
        let engine = try await InferenceEngine.loadModel(config) { progress in
            Task { await loadProgressCollector.addProgress(progress) }
        }
        
        let loadTime = Date().timeIntervalSince(startTime)
        let loadProgress = await loadProgressCollector.getProgressValues()
        
        print("‚úÖ [DIFFUSION TEST] Diffusion model loaded successfully!")
        print("   - Load time: \(String(format: "%.2f", loadTime)) seconds")
        print("   - Progress points: \(loadProgress.count)")
        
        // Test diffusion-specific features
        XCTAssertTrue(InferenceEngine.supportedFeatures.contains(.multiModalInput), "Multi-modal input should be available for diffusion")
        
        // Test image generation prompt
        let testPrompt = "A beautiful sunset over the ocean"
        print("üìù [DIFFUSION TEST] Test prompt: \"\(testPrompt)\"")
        
        let response = try await engine.generate(testPrompt, params: GenerateParams(maxTokens: 77, temperature: 0.8))
        
        print("‚úÖ [DIFFUSION TEST] Diffusion generation successful!")
        print("   - Response length: \(response.count)")
        
        XCTAssertFalse(response.isEmpty, "Diffusion response should not be empty")
        
        // Test model unloading
        print("\nüßπ [DIFFUSION TEST] Testing diffusion model unloading...")
        engine.unload()
        print("‚úÖ [DIFFUSION TEST] Diffusion model unloaded successfully")
        
        print("‚úÖ [DIFFUSION TEST] Diffusion model test completed successfully!")
    }
    
    // MARK: - Quantization Tests
    
    func testQuantizedModelInference() async throws {
        guard ProcessInfo.processInfo.environment["RUN_REAL_MODEL_TESTS"] != nil else {
            throw XCTSkip("Skipping real model test: RUN_REAL_MODEL_TESTS not set")
        }
        print("\n‚ö° [QUANTIZATION TEST] Testing quantized model inference...")
        
        // Check if quantization features are available
        guard InferenceEngine.supportedFeatures.contains(.quantizationSupport) else {
            print("‚ö†Ô∏è [QUANTIZATION TEST] Quantization features not available, skipping test")
            return
        }
        
        // Test with FP16 quantized model
        let config = ModelRegistry.llama32_3B_fp16
        print("üìã [QUANTIZATION TEST] Using model: \(config.name) (\(config.hubId))")
        
        let loadProgressCollector = ProgressCollector()
        let startTime = Date()
        
        // Load the quantized model
        let engine = try await InferenceEngine.loadModel(config) { progress in
            Task { await loadProgressCollector.addProgress(progress) }
        }
        
        let loadTime = Date().timeIntervalSince(startTime)
        let loadProgress = await loadProgressCollector.getProgressValues()
        
        print("‚úÖ [QUANTIZATION TEST] Quantized model loaded successfully!")
        print("   - Load time: \(String(format: "%.2f", loadTime)) seconds")
        print("   - Progress points: \(loadProgress.count)")
        print("   - Quantization: \(config.quantization ?? "unknown")")
        
        // Test text generation with quantized model
        let testPrompt = "Explain the benefits of model quantization in one sentence."
        print("üìù [QUANTIZATION TEST] Test prompt: \"\(testPrompt)\"")
        
        let response = try await engine.generate(testPrompt, params: GenerateParams(maxTokens: 100, temperature: 0.7))
        
        print("‚úÖ [QUANTIZATION TEST] Quantized model generation successful!")
        print("   - Response: \"\(response)\"")
        
        XCTAssertFalse(response.isEmpty, "Quantized model response should not be empty")
        
        // Test model unloading
        print("\nüßπ [QUANTIZATION TEST] Testing quantized model unloading...")
        engine.unload()
        print("‚úÖ [QUANTIZATION TEST] Quantized model unloaded successfully")
        
        print("‚úÖ [QUANTIZATION TEST] Quantized model test completed successfully!")
    }
    
    // MARK: - Performance Monitoring Tests
    
    func testPerformanceMonitoring() async throws {
        guard ProcessInfo.processInfo.environment["RUN_REAL_MODEL_TESTS"] != nil else {
            throw XCTSkip("Skipping real model test: RUN_REAL_MODEL_TESTS not set")
        }
        print("\nüìä [PERFORMANCE TEST] Testing performance monitoring...")
        
        // Check if performance monitoring features are available
        guard InferenceEngine.supportedFeatures.contains(.performanceMonitoring) else {
            print("‚ö†Ô∏è [PERFORMANCE TEST] Performance monitoring features not available, skipping test")
            return
        }
        
        let config = ModelRegistry.qwen05B
        print("üìã [PERFORMANCE TEST] Using model: \(config.name)")
        
        let startTime = Date()
        
        // Load the model
        let engine = try await InferenceEngine.loadModel(config) { _ in }
        
        let loadTime = Date().timeIntervalSince(startTime)
        
        // Get engine status
        let status = engine.status
        print("‚úÖ [PERFORMANCE TEST] Engine status retrieved:")
        print("   - Model loaded: \(status.isModelLoaded)")
        print("   - MLX available: \(status.mlxAvailable)")
        print("   - Load time: \(String(format: "%.2f", loadTime)) seconds")
        
        XCTAssertTrue(status.isModelLoaded, "Model should be loaded")
        XCTAssertTrue(status.mlxAvailable, "MLX should be available")
        
        // Test generation with performance monitoring
        let generateStartTime = Date()
        let response = try await engine.generate("Test performance monitoring", params: .init(maxTokens: 50))
        let generateTime = Date().timeIntervalSince(generateStartTime)
        
        print("‚úÖ [PERFORMANCE TEST] Generation performance:")
        print("   - Generation time: \(String(format: "%.2f", generateTime)) seconds")
        print("   - Response length: \(response.count)")
        
        XCTAssertTrue(generateTime > 0, "Generation time should be positive")
        
        // Test model unloading
        engine.unload()
        print("‚úÖ [PERFORMANCE TEST] Performance monitoring test completed successfully!")
    }
    
    // MARK: - Model Registry Tests
    
    func testModelRegistryComprehensive() async throws {
        guard ProcessInfo.processInfo.environment["RUN_REAL_MODEL_TESTS"] != nil else {
            throw XCTSkip("Skipping real model test: RUN_REAL_MODEL_TESTS not set")
        }
        print("\nüìö [REGISTRY TEST] Testing comprehensive model registry...")
        
        // Test all model types
        let allModels = ModelRegistry.allModels
        print("üìã [REGISTRY TEST] Total models in registry: \(allModels.count)")
        
        // Group models by type
        let llmModels = allModels.filter { ModelRegistry.getModelType($0) == .llm }
        let vlmModels = allModels.filter { ModelRegistry.getModelType($0) == .vlm }
        let embedderModels = allModels.filter { ModelRegistry.getModelType($0) == .embedder }
        let diffusionModels = allModels.filter { ModelRegistry.getModelType($0) == .diffusion }
        
        print("üìä [REGISTRY TEST] Model distribution:")
        print("   - LLM models: \(llmModels.count)")
        print("   - VLM models: \(vlmModels.count)")
        print("   - Embedder models: \(embedderModels.count)")
        print("   - Diffusion models: \(diffusionModels.count)")
        
        // Test model search functionality
        let searchResults = ModelRegistry.searchModels(query: "qwen", type: .llm)
        print("üîç [REGISTRY TEST] Search results for 'qwen': \(searchResults.count) models")
        
        XCTAssertTrue(searchResults.count > 0, "Should find Qwen models")
        
        // Test model recommendations
        let mobileModels = ModelRegistry.getRecommendedModels(for: .mobileDevelopment)
        let qualityModels = ModelRegistry.getRecommendedModels(for: .highQualityGeneration)
        
        print("üì± [REGISTRY TEST] Mobile models: \(mobileModels.count)")
        print("üéØ [REGISTRY TEST] Quality models: \(qualityModels.count)")
        
        XCTAssertTrue(mobileModels.count > 0, "Should have mobile-optimized models")
        XCTAssertTrue(qualityModels.count > 0, "Should have high-quality models")
        
        print("‚úÖ [REGISTRY TEST] Model registry test completed successfully!")
    }
    
    // MARK: - Error Handling Tests
    
    func testErrorHandling() async throws {
        guard ProcessInfo.processInfo.environment["RUN_REAL_MODEL_TESTS"] != nil else {
            throw XCTSkip("Skipping real model test: RUN_REAL_MODEL_TESTS not set")
        }
        print("\n‚ö†Ô∏è [ERROR TEST] Testing error handling...")
        
        // Test with invalid model configuration
        let invalidConfig = ModelConfiguration(
            name: "Invalid Model",
            hubId: "invalid/model/that/does/not/exist",
            description: "This model does not exist",
            maxTokens: 100
        )
        
        do {
            let _ = try await InferenceEngine.loadModel(invalidConfig) { _ in }
            XCTFail("Should have thrown an error for invalid model")
        } catch {
            print("‚úÖ [ERROR TEST] Correctly caught error: \(error)")
            XCTAssertTrue(error is MLXEngineError, "Error should be MLXEngineError")
        }
        
        // Test with unsupported features
        let engine = try await InferenceEngine.loadModel(ModelRegistry.qwen05B) { _ in }
        
        do {
            try await engine.loadLoRAAdapter(from: URL(string: "file:///invalid")!)
            XCTFail("Should have thrown an error for unsupported LoRA")
        } catch {
            print("‚úÖ [ERROR TEST] Correctly caught LoRA error: \(error)")
        }
        
        engine.unload()
        print("‚úÖ [ERROR TEST] Error handling test completed successfully!")
    }
}

// MARK: - Helper Classes

/// Helper class to collect progress values during async operations
actor ProgressCollector {
    private var progressValues: [Double] = []
    
    func addProgress(_ progress: Double) {
        progressValues.append(progress)
    }
    
    func getProgressValues() -> [Double] {
        return progressValues
    }
}

#else

final class MLXIntegrationTests: XCTestCase {
    
    func testMLXDependenciesNotAvailable() {
        print("\n‚ö†Ô∏è [MLX TEST] MLX dependencies not available, skipping integration tests")
        // This test will run when MLX dependencies are not available
        XCTAssertTrue(true)
    }
}

#endif 