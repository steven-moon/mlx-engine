import Foundation
import XCTest

@testable import MLXEngine

/// Real-world tests that download and test actual models from HuggingFace
final class RealWorldModelTests: XCTestCase {

  // MARK: - Test Configuration

  /// Test timeout for model downloads and inference
  private let testTimeout: TimeInterval = 300  // 5 minutes

  /// Whether to run expensive tests (model downloads)
  private var shouldRunExpensiveTests: Bool {
    // Set to true to run full model download tests
    // Set to false for faster CI/CD runs
    return ProcessInfo.processInfo.environment["RUN_EXPENSIVE_TESTS"] == "true"
  }

  // MARK: - LLM Model Tests

  func testRealLLMModelDownloadAndInference() async throws {
    guard shouldRunExpensiveTests else {
      print("‚ö†Ô∏è [REAL WORLD TEST] Skipping expensive LLM model download test")
      return
    }

    print("\nü§ñ [REAL WORLD LLM TEST] Testing real LLM model download and inference...")

    // Test with a small, fast LLM model
    let config = ModelRegistry.qwen05B
    print("üìã [REAL WORLD LLM TEST] Using model: \(config.name) (\(config.hubId))")

    let loadProgressCollector = ProgressCollector()
    let startTime = Date()

    // Download and load the model
    let engine = try await InferenceEngine.loadModel(config) { progress in
      Task { await loadProgressCollector.addProgress(progress) }
    }

    let loadTime = Date().timeIntervalSince(startTime)
    let loadProgress = await loadProgressCollector.getProgressValues()

    print("‚úÖ [REAL WORLD LLM TEST] Model loaded successfully!")
    print("   - Load time: \(String(format: "%.2f", loadTime)) seconds")
    print("   - Progress points: \(loadProgress.count)")

    // Test text generation
    let testPrompts = [
      "Hello, how are you?",
      "What is the capital of France?",
      "Explain quantum computing in simple terms.",
    ]

    for (index, prompt) in testPrompts.enumerated() {
      print("üìù [REAL WORLD LLM TEST] Test prompt \(index + 1): \"\(prompt)\"")

      let generateStartTime = Date()
      let response = try await engine.generate(
        prompt, params: .init(maxTokens: 100, temperature: 0.7))
      let generateTime = Date().timeIntervalSince(generateStartTime)

      print("‚úÖ [REAL WORLD LLM TEST] Generation \(index + 1) successful!")
      print("   - Response: \"\(response)\"")
      print("   - Generation time: \(String(format: "%.2f", generateTime)) seconds")

      XCTAssertFalse(response.isEmpty, "Response should not be empty")
      XCTAssertGreaterThan(response.count, 10, "Response should be substantial")
    }

    // Test streaming generation
    print("üì° [REAL WORLD LLM TEST] Testing streaming generation...")
    let streamPrompt = "Write a short story about a robot learning to paint."
    let streamStartTime = Date()

    var streamedResponse = ""
    var tokenCount = 0

    for try await token in engine.stream(
      streamPrompt, params: .init(maxTokens: 50, temperature: 0.8))
    {
      streamedResponse += token
      tokenCount += 1
      print("   Token \(tokenCount): \"\(token)\"")
    }

    let streamTime = Date().timeIntervalSince(streamStartTime)
    print("‚úÖ [REAL WORLD LLM TEST] Streaming successful!")
    print("   - Streamed response: \"\(streamedResponse)\"")
    print("   - Tokens generated: \(tokenCount)")
    print("   - Stream time: \(String(format: "%.2f", streamTime)) seconds")

    XCTAssertFalse(streamedResponse.isEmpty, "Streamed response should not be empty")
    XCTAssertGreaterThan(tokenCount, 0, "Should generate at least one token")

    // Test model unloading
    print("\nüßπ [REAL WORLD LLM TEST] Testing model unloading...")
    engine.unload()
    print("‚úÖ [REAL WORLD LLM TEST] Model unloaded successfully")

    print("‚úÖ [REAL WORLD LLM TEST] LLM model test completed successfully!")
  }

  // MARK: - VLM Model Tests

  func testRealVLMModelDownloadAndInference() async throws {
    guard shouldRunExpensiveTests else {
      print("‚ö†Ô∏è [REAL WORLD TEST] Skipping expensive VLM model download test")
      return
    }

    print("\nüñºÔ∏è [REAL WORLD VLM TEST] Testing real VLM model download and inference...")

    // Test with LLaVA model
    let config = ModelRegistry.llava16_3B
    print("üìã [REAL WORLD VLM TEST] Using model: \(config.name) (\(config.hubId))")

    let loadProgressCollector = ProgressCollector()
    let startTime = Date()

    // Download and load the VLM model
    let engine = try await InferenceEngine.loadModel(config) { progress in
      Task { await loadProgressCollector.addProgress(progress) }
    }

    let loadTime = Date().timeIntervalSince(startTime)
    let loadProgress = await loadProgressCollector.getProgressValues()

    print("‚úÖ [REAL WORLD VLM TEST] VLM model loaded successfully!")
    print("   - Load time: \(String(format: "%.2f", loadTime)) seconds")
    print("   - Progress points: \(loadProgress.count)")

    // Test text-only generation (VLM models can also do text generation)
    let textPrompt =
      "Describe what you see in this image: [No image provided, please respond with a general description of your capabilities]"
    print("üìù [REAL WORLD VLM TEST] Test text prompt: \"\(textPrompt)\"")

    let response = try await engine.generate(
      textPrompt, params: .init(maxTokens: 100, temperature: 0.7))

    print("‚úÖ [REAL WORLD VLM TEST] VLM text generation successful!")
    print("   - Response: \"\(response)\"")

    XCTAssertFalse(response.isEmpty, "VLM text response should not be empty")

    // Test multi-modal input (placeholder for when implemented)
    if InferenceEngine.supportedFeatures.contains(.multiModalInput) {
      print("üñºÔ∏è [REAL WORLD VLM TEST] Testing multi-modal input...")

      let multiModalInput = InferenceEngine.MultiModalInput.text("Describe this image")

      do {
        let multiModalResponse = try await engine.generateWithMultiModalInput(multiModalInput)
        print("‚úÖ [REAL WORLD VLM TEST] Multi-modal generation successful!")
        print("   - Response: \"\(multiModalResponse)\"")
      } catch {
        print("‚ö†Ô∏è [REAL WORLD VLM TEST] Multi-modal generation not yet implemented: \(error)")
      }
    }

    // Test model unloading
    print("\nüßπ [REAL WORLD VLM TEST] Testing VLM model unloading...")
    engine.unload()
    print("‚úÖ [REAL WORLD VLM TEST] VLM model unloaded successfully")

    print("‚úÖ [REAL WORLD VLM TEST] VLM model test completed successfully!")
  }

  // MARK: - Embedding Model Tests

  func testRealEmbeddingModelDownloadAndInference() async throws {
    guard shouldRunExpensiveTests else {
      print("‚ö†Ô∏è [REAL WORLD TEST] Skipping expensive embedding model download test")
      return
    }

    print("\nüîç [REAL WORLD EMBEDDING TEST] Testing real embedding model download and inference...")

    // Test with BGE embedding model
    let config = ModelRegistry.bgeSmallEn
    print("üìã [REAL WORLD EMBEDDING TEST] Using model: \(config.name) (\(config.hubId))")

    let loadProgressCollector = ProgressCollector()
    let startTime = Date()

    // Download and load the embedding model
    let engine = try await InferenceEngine.loadModel(config) { progress in
      Task { await loadProgressCollector.addProgress(progress) }
    }

    let loadTime = Date().timeIntervalSince(startTime)
    let loadProgress = await loadProgressCollector.getProgressValues()

    print("‚úÖ [REAL WORLD EMBEDDING TEST] Embedding model loaded successfully!")
    print("   - Load time: \(String(format: "%.2f", loadTime)) seconds")
    print("   - Progress points: \(loadProgress.count)")

    // Test embedding generation
    let testTexts = [
      "The quick brown fox jumps over the lazy dog.",
      "Machine learning is a subset of artificial intelligence.",
      "The weather is sunny today.",
    ]

    for (index, text) in testTexts.enumerated() {
      print("üìù [REAL WORLD EMBEDDING TEST] Test text \(index + 1): \"\(text)\"")

      let generateStartTime = Date()
      let response = try await engine.generate(
        text, params: .init(maxTokens: 512, temperature: 0.0))
      let generateTime = Date().timeIntervalSince(generateStartTime)

      print("‚úÖ [REAL WORLD EMBEDDING TEST] Embedding \(index + 1) successful!")
      print("   - Response length: \(response.count)")
      print("   - Generation time: \(String(format: "%.2f", generateTime)) seconds")

      XCTAssertFalse(response.isEmpty, "Embedding response should not be empty")
    }

    // Test model unloading
    print("\nüßπ [REAL WORLD EMBEDDING TEST] Testing embedding model unloading...")
    engine.unload()
    print("‚úÖ [REAL WORLD EMBEDDING TEST] Embedding model unloaded successfully")

    print("‚úÖ [REAL WORLD EMBEDDING TEST] Embedding model test completed successfully!")
  }

  // MARK: - Diffusion Model Tests

  func testRealDiffusionModelDownloadAndInference() async throws {
    guard shouldRunExpensiveTests else {
      print("‚ö†Ô∏è [REAL WORLD TEST] Skipping expensive diffusion model download test")
      return
    }

    print("\nüé® [REAL WORLD DIFFUSION TEST] Testing real diffusion model download and inference...")

    // Test with Stable Diffusion model
    let config = ModelRegistry.stableDiffusionXL
    print("üìã [REAL WORLD DIFFUSION TEST] Using model: \(config.name) (\(config.hubId))")

    let loadProgressCollector = ProgressCollector()
    let startTime = Date()

    // Download and load the diffusion model
    let engine = try await InferenceEngine.loadModel(config) { progress in
      Task { await loadProgressCollector.addProgress(progress) }
    }

    let loadTime = Date().timeIntervalSince(startTime)
    let loadProgress = await loadProgressCollector.getProgressValues()

    print("‚úÖ [REAL WORLD DIFFUSION TEST] Diffusion model loaded successfully!")
    print("   - Load time: \(String(format: "%.2f", loadTime)) seconds")
    print("   - Progress points: \(loadProgress.count)")

    // Test image generation (placeholder for when implemented)
    if InferenceEngine.supportedFeatures.contains(.diffusionModels) {
      print("üé® [REAL WORLD DIFFUSION TEST] Testing image generation...")

      let testPrompts = [
        "A beautiful sunset over mountains",
        "A cute cat sitting on a windowsill",
        "A futuristic city skyline at night",
      ]

      for (index, prompt) in testPrompts.enumerated() {
        print("üìù [REAL WORLD DIFFUSION TEST] Test prompt \(index + 1): \"\(prompt)\"")

        do {
          let generateStartTime = Date()
          let imageData = try await engine.generateImage(from: prompt, params: .init(maxTokens: 77))
          let generateTime = Date().timeIntervalSince(generateStartTime)

          print("‚úÖ [REAL WORLD DIFFUSION TEST] Image generation \(index + 1) successful!")
          print("   - Image data size: \(imageData.count) bytes")
          print("   - Generation time: \(String(format: "%.2f", generateTime)) seconds")

          XCTAssertGreaterThan(imageData.count, 0, "Generated image should have data")
        } catch {
          print("‚ö†Ô∏è [REAL WORLD DIFFUSION TEST] Image generation not yet implemented: \(error)")
        }
      }
    }

    // Test model unloading
    print("\nüßπ [REAL WORLD DIFFUSION TEST] Testing diffusion model unloading...")
    engine.unload()
    print("‚úÖ [REAL WORLD DIFFUSION TEST] Diffusion model unloaded successfully")

    print("‚úÖ [REAL WORLD DIFFUSION TEST] Diffusion model test completed successfully!")
  }

  // MARK: - LoRA Adapter Tests

  func testRealLoRAAdapterSupport() async throws {
    guard shouldRunExpensiveTests else {
      print("‚ö†Ô∏è [REAL WORLD TEST] Skipping expensive LoRA adapter test")
      return
    }

    print("\nüîß [REAL WORLD LORA TEST] Testing real LoRA adapter support...")

    // Test with a base model
    let config = ModelRegistry.qwen05B
    print("üìã [REAL WORLD LORA TEST] Using base model: \(config.name) (\(config.hubId))")

    let engine = try await InferenceEngine.loadModel(config) { _ in }

    // Test LoRA adapter loading (placeholder for when implemented)
    if InferenceEngine.supportedFeatures.contains(.loraAdapters) {
      print("üîß [REAL WORLD LORA TEST] Testing LoRA adapter loading...")

      // Create a fake LoRA adapter file for testing
      let fakeLoRAURL = URL(fileURLWithPath: "/tmp/fake-lora-adapter.safetensors")

      do {
        try await engine.loadLoRAAdapter(from: fakeLoRAURL)
        print("‚úÖ [REAL WORLD LORA TEST] LoRA adapter loading successful!")
      } catch {
        print("‚ö†Ô∏è [REAL WORLD LORA TEST] LoRA adapter loading not yet implemented: \(error)")
      }

      // Test LoRA adapter application
      print("üîß [REAL WORLD LORA TEST] Testing LoRA adapter application...")

      do {
        try engine.applyLoRAAdapter(named: "test-adapter")
        print("‚úÖ [REAL WORLD LORA TEST] LoRA adapter application successful!")
      } catch {
        print("‚ö†Ô∏è [REAL WORLD LORA TEST] LoRA adapter application not yet implemented: \(error)")
      }
    } else {
      print("‚ö†Ô∏è [REAL WORLD LORA TEST] LoRA adapters not supported by this engine")
    }

    // Test model unloading
    print("\nüßπ [REAL WORLD LORA TEST] Testing model unloading...")
    engine.unload()
    print("‚úÖ [REAL WORLD LORA TEST] Model unloaded successfully")

    print("‚úÖ [REAL WORLD LORA TEST] LoRA adapter test completed successfully!")
  }

  // MARK: - Model Training Tests

  func testRealModelTrainingSupport() async throws {
    guard shouldRunExpensiveTests else {
      print("‚ö†Ô∏è [REAL WORLD TEST] Skipping expensive model training test")
      return
    }

    print("\nüéì [REAL WORLD TRAINING TEST] Testing real model training support...")

    // Test model training capabilities
    if ModelTrainer.isTrainingSupported {
      print("üéì [REAL WORLD TRAINING TEST] Model training is supported!")

      let config = TrainingConfig(
        learningRate: 0.001,
        epochs: 5,
        batchSize: 2
      )

      let trainer = ModelTrainer(config: config)

      // Test training with sample data
      let trainingData = TrainingData(
        inputs: [
          "Hello, how are you?",
          "What is the weather like?",
          "Tell me a joke.",
        ],
        targets: [
          "I'm doing well, thank you for asking!",
          "I don't have access to real-time weather information.",
          "Why don't scientists trust atoms? Because they make up everything!",
        ]
      )

      print("üéì [REAL WORLD TRAINING TEST] Testing model fine-tuning...")

      do {
        let _ = try await trainer.fineTune(
          model: "mock-model",
          data: trainingData
        ) { metrics in
          print(
            "üìä [REAL WORLD TRAINING TEST] Training progress - Epoch \(metrics.epoch), Loss: \(metrics.trainingLoss)"
          )
        }
        print("‚úÖ [REAL WORLD TRAINING TEST] Model fine-tuning successful!")
      } catch {
        print("‚ö†Ô∏è [REAL WORLD TRAINING TEST] Model fine-tuning not yet implemented: \(error)")
      }

      // Test model evaluation
      print("üéì [REAL WORLD TRAINING TEST] Testing model evaluation...")

      do {
        let metrics = try await trainer.evaluate(
          model: "mock-model",
          testData: trainingData
        )
        print("‚úÖ [REAL WORLD TRAINING TEST] Model evaluation successful!")
        print("   - Test loss: \(metrics.testLoss)")
        print("   - Evaluation time: \(String(format: "%.2f", metrics.evaluationTime)) seconds")
      } catch {
        print("‚ö†Ô∏è [REAL WORLD TRAINING TEST] Model evaluation not yet implemented: \(error)")
      }
    } else {
      print("‚ö†Ô∏è [REAL WORLD TRAINING TEST] Model training is not supported by this engine")
    }

    print("‚úÖ [REAL WORLD TRAINING TEST] Model training test completed successfully!")
  }

  // MARK: - Performance Benchmark Tests

  func testRealWorldPerformanceBenchmarks() async throws {
    guard shouldRunExpensiveTests else {
      print("‚ö†Ô∏è [REAL WORLD TEST] Skipping expensive performance benchmark test")
      return
    }

    print("\n‚è±Ô∏è [REAL WORLD BENCHMARK TEST] Testing real-world performance benchmarks...")

    // Test with different model sizes
    let testModels = [
      ModelRegistry.qwen05B,  // Small model
      ModelRegistry.llama32_3B,  // Medium model
      ModelRegistry.mistral7B,  // Large model
    ]

    for (index, config) in testModels.enumerated() {
      print("üìä [REAL WORLD BENCHMARK TEST] Benchmarking model \(index + 1): \(config.name)")

      let loadStartTime = Date()
      let engine = try await InferenceEngine.loadModel(config) { _ in }
      let loadTime = Date().timeIntervalSince(loadStartTime)

      print("   - Load time: \(String(format: "%.2f", loadTime)) seconds")

      // Test generation performance
      let testPrompt = "The quick brown fox jumps over the lazy dog."
      let generateStartTime = Date()

      let response = try await engine.generate(
        testPrompt, params: .init(maxTokens: 50, temperature: 0.7))
      let generateTime = Date().timeIntervalSince(generateStartTime)

      print("   - Generation time: \(String(format: "%.2f", generateTime)) seconds")
      print("   - Response length: \(response.count) characters")

      // Calculate tokens per second (approximate)
      let tokensPerSecond =
        Double(response.components(separatedBy: .whitespacesAndNewlines).count) / generateTime
      print("   - Approximate tokens per second: \(String(format: "%.1f", tokensPerSecond))")

      // Test memory usage
      let status = engine.status
      print("   - MLX available: \(status.mlxAvailable)")
      print("   - Model loaded: \(status.isModelLoaded)")

      engine.unload()
    }

    print("‚úÖ [REAL WORLD BENCHMARK TEST] Performance benchmarks completed successfully!")
  }

  // MARK: - Error Handling Tests

  func testRealWorldErrorHandling() async throws {
    print("\n‚ö†Ô∏è [REAL WORLD ERROR TEST] Testing real-world error handling...")

    // Test with unsupported features
    let engine = try await InferenceEngine.loadModel(ModelRegistry.qwen05B) { _ in }

    // Test LoRA adapter with unsupported model
    do {
      try await engine.loadLoRAAdapter(from: URL(string: "file:///invalid")!)
      XCTFail("Should have thrown an error for unsupported LoRA")
    } catch {
      print("‚úÖ [REAL WORLD ERROR TEST] Correctly caught LoRA error: \(error)")
    }

    // Test multi-modal input with non-VLM model
    do {
      let input = InferenceEngine.MultiModalInput.text("Test")
      let _ = try await engine.generateWithMultiModalInput(input)
      XCTFail("Should have thrown an error for multi-modal input on non-VLM model")
    } catch {
      print("‚úÖ [REAL WORLD ERROR TEST] Correctly caught multi-modal error: \(error)")
    }

    // Test image generation with non-diffusion model
    do {
      let _ = try await engine.generateImage(from: "Test prompt")
      XCTFail("Should have thrown an error for image generation on non-diffusion model")
    } catch {
      print("‚úÖ [REAL WORLD ERROR TEST] Correctly caught image generation error: \(error)")
    }

    engine.unload()
    print("‚úÖ [REAL WORLD ERROR TEST] Error handling test completed successfully!")
  }
}
